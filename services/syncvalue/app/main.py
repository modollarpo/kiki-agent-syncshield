"""
SyncValue â€“ LTV Prediction Engine API
"""

from fastapi import FastAPI, HTTPException, Response, Body, Depends, status, BackgroundTasks
from pydantic import BaseModel, Field, ValidationError
from typing import Any, Dict, Optional

import torch
import os
import logging
from ltv_dRNN import LTVdRNN
from feature_extractor import extract_features
from training_loop import train_model
from inference import predict as model_predict
# from services.syncvalue.app.handlers_ltv import router as ltv_router
import threading
import grpc
from concurrent import futures
# from shared.types.python.value import value_pb2_grpc, value_pb2
# from services.syncvalue.internal.usecases.ltv_inference import predict_ltv
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from passlib.context import CryptContext
import jwt, uuid
from datetime import datetime, timedelta
# from fastapi_limiter import FastAPILimiter, RateLimiter  # Rate limiting disabled for now
# import aioredis

# Prometheus metrics
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

# OpenTelemetry tracing
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter


app = FastAPI(title="SyncValue LTV Prediction", description="LTV prediction with dRNN, PyTorch, and zero-shot learning.")

# Instrument FastAPI with OpenTelemetry
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)
FastAPIInstrumentor.instrument_app(app)
span_processor = BatchSpanProcessor(ConsoleSpanExporter())
trace.get_tracer_provider().add_span_processor(span_processor)

# Prometheus metrics
REQUEST_COUNT = Counter("syncvalue_requests_total", "Total requests", ["endpoint", "method"])
ERROR_COUNT = Counter("syncvalue_errors_total", "Total errors", ["endpoint"])
LTV_PREDICTIONS = Counter("syncvalue_ltv_predictions_total", "Total LTV predictions")
REQUEST_LATENCY = Histogram("syncvalue_request_latency_seconds", "Request latency", ["endpoint"])

logger = logging.getLogger("syncvalue")

# Initialize configuration
from shared.config import SyncValueConfig
config = SyncValueConfig()
logging.basicConfig(level=getattr(logging, config.log_level.upper(), logging.INFO))

# Initialize enterprise components
from shared.health import HealthChecker, ServiceHealth, DependencyHealth, HealthStatus
health_checker = HealthChecker(service_name=config.service_name, version=config.version)

# Model config from environment
MODEL_PATH = config.ltv_model_path
INPUT_SIZE = config.model_input_size
HIDDEN_SIZE = config.model_hidden_size
NUM_LAYERS = 2
OUTPUT_SIZE = 1

# Enhanced health check
async def check_pytorch_model():
    """Check if PyTorch model is available"""
    import time
    start = time.time()
    try:
        if os.path.exists(MODEL_PATH):
            status = HealthStatus.HEALTHY
            error = None
        else:
            status = HealthStatus.DEGRADED
            error = "Model file not found, will train on first request"
        
        return DependencyHealth(
            name="pytorch_model",
            status=status,
            response_time_ms=round((time.time() - start) * 1000, 2),
            error=error,
            last_check=datetime.utcnow().isoformat() + "Z"
        )
    except Exception as e:
        return DependencyHealth(
            name="pytorch_model",
            status=HealthStatus.UNHEALTHY,
            response_time_ms=round((time.time() - start) * 1000, 2),
            error=str(e),
            last_check=datetime.utcnow().isoformat() + "Z"
        )

health_checker.register_dependency("pytorch_model", check_pytorch_model)

@app.get("/health", response_model=ServiceHealth, tags=["monitoring"])
async def enhanced_health():
    """Enhanced health check with PyTorch model status and metrics"""
    metrics = {
        "ltv_predictions": LTV_PREDICTIONS._value.get(),
        "model_path": MODEL_PATH,
        "model_loaded": os.path.exists(MODEL_PATH),
        "pytorch_version": torch.__version__
    }
    return await health_checker.get_health(metrics=metrics)

def load_model():
    model = LTVdRNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)
    if os.path.exists(MODEL_PATH):
        model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
    return model

class PredictRequest(BaseModel):
    user_id: str = Field(..., json_schema_extra={"example": "user_123"})
    features: Dict[str, Any] = Field(..., json_schema_extra={"example": {"f1": 0.5, "f2": 1.2}})

class PredictResponse(BaseModel):
    ltv: float = Field(..., json_schema_extra={"example": 123.45})
    user_id: str = Field(..., json_schema_extra={"example": "user_123"})

class TrainRequest(BaseModel):
    dataset_path: str = Field(..., json_schema_extra={"example": "/data/ltv.csv"})
    params: Dict[str, Any] = Field(default_factory=dict)

class TrainResponse(BaseModel):
    status: str = Field(..., json_schema_extra={"example": "training_started"})
    dataset: str = Field(..., json_schema_extra={"example": "/data/ltv.csv"})

class MetricsResponse(BaseModel):
    accuracy: float = Field(..., json_schema_extra={"example": 0.95})
    loss: float = Field(..., json_schema_extra={"example": 0.1})

@app.post("/predict-ltv", response_model=PredictResponse, responses={
    200: {"description": "LTV prediction", "content": {"application/json": {"example": {"ltv": 123.45, "user_id": "user_123"}}}},
    400: {"description": "Invalid input"},
    500: {"description": "Model error"}
})
def predict_ltv(req: PredictRequest):
    REQUEST_COUNT.labels(endpoint="/predict-ltv", method="POST").inc()
    with REQUEST_LATENCY.labels(endpoint="/predict-ltv").time():
        with tracer.start_as_current_span("predict_ltv"):
            try:
                # Validate features strictly before model logic
                if not isinstance(req.features, dict) or not req.features:
                    ERROR_COUNT.labels(endpoint="/predict-ltv").inc()
                    raise HTTPException(status_code=400, detail="Features must be a non-empty dict.")
                feats = extract_features(req.features)
                # Ensure features are a 2D tensor: (batch, seq_len, input_size)
                # For demo, treat as batch=1, seq_len=1, input_size=INPUT_SIZE
                feat_vec = list(feats.values())[:INPUT_SIZE] + [0.0]*(INPUT_SIZE - len(feats))
                x = torch.tensor([ [feat_vec] ], dtype=torch.float32)  # shape: (1,1,INPUT_SIZE)
                model = load_model()
                y_pred = model_predict(model, x)
                ltv = float(y_pred.item())
                logger.info(f"Predicted LTV for {req.user_id}: {ltv}")
                LTV_PREDICTIONS.inc()
                return {"ltv": ltv, "user_id": req.user_id}
            except HTTPException as he:
                logger.error(f"Validation error: {he.detail}")
                ERROR_COUNT.labels(endpoint="/predict-ltv").inc()
                raise
            except ValidationError as ve:
                logger.error(f"Validation error: {ve}")
                ERROR_COUNT.labels(endpoint="/predict-ltv").inc()
                raise HTTPException(status_code=400, detail=str(ve))
            except Exception as e:
                logger.error(f"Model error: {e}")
                ERROR_COUNT.labels(endpoint="/predict-ltv").inc()
                raise HTTPException(status_code=500, detail="Model error.")

@app.post("/train", response_model=TrainResponse, responses={
    200: {"description": "Training started", "content": {"application/json": {"example": {"status": "training_started", "dataset": "/data/ltv.csv"}}}},
    400: {"description": "Invalid input"},
    500: {"description": "Training error"}
})
def train(req: TrainRequest):
    try:
        # TODO: Load dataset, run training loop, save model
        logger.info(f"Training started on {req.dataset_path}")
        return {"status": "training_started", "dataset": req.dataset_path}
    except Exception as e:
        logger.error(f"Training error: {e}")
        raise HTTPException(status_code=500, detail="Training error.")


# Include the new LTV router
# app.include_router(ltv_router)  # Commented out - router not available in container

# Prometheus /metrics endpoint
@app.get("/metrics")
def prometheus_metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.get("/healthz")
def health_check():
    return {"status": "ok"}

# gRPC server implementation (disabled - missing protobuf files)
# class LTVServiceServicer(value_pb2_grpc.LTVServiceServicer):
#     def PredictLTV(self, request, context):
#         ltv = predict_ltv(dict(request.features))
#         return value_pb2.PredictLTVResponse(user_id=request.user_id, ltv=ltv)
#     def TrainLTV(self, request, context):
#         # TODO: Implement training logic
#         return value_pb2.TrainLTVResponse(status="training started")

# def serve_grpc():
#     server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
#     value_pb2_grpc.add_LTVServiceServicer_to_server(LTVServiceServicer(), server)
#     server.add_insecure_port('[::]:50052')
#     server.start()
#     server.wait_for_termination()

# @app.on_event("startup")
# def start_grpc_server():
#     threading.Thread(target=serve_grpc, daemon=True).start()

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
USER_DB = {"admin": {"username": "admin", "password": pwd_context.hash("adminpass"), "role": "admin"}}
SECRET_KEY = "supersecretkey"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60
EMAIL_VERIFICATION = {}
PASSWORD_RESET = {}
AUDIT_LOG = []

@app.on_event("startup")
async def startup():
    # redis = await aioredis.create_redis_pool("redis://localhost")  # Rate limiting disabled
    # await FastAPILimiter.init(redis)  # Rate limiting disabled
    logger.info("SyncValue LTV service ready.")

security = HTTPBearer()

def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    try:
        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except Exception:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

def require_role(role: str):
    def role_checker(user=Depends(get_current_user)):
        if user.get("role") != role:
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Insufficient permissions")
        return user
    return role_checker

@app.post("/register")
def register(username: str = Body(...), password: str = Body(...), role: str = Body("analyst")):
    if username in USER_DB:
        return {"error": "User already exists"}
    USER_DB[username] = {"username": username, "password": pwd_context.hash(password), "role": role}
    return {"msg": "User registered"}

@app.post("/login")  # Rate limiting disabled
def login(username: str = Body(...), password: str = Body(...)):
    user = USER_DB.get(username)
    if not user or not pwd_context.verify(password, user["password"]):
        return {"error": "Invalid credentials"}
    access_token = jwt.encode({
        "sub": username,
        "role": user["role"],
        "exp": datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    }, SECRET_KEY, algorithm=ALGORITHM)
    AUDIT_LOG.append({"user": username, "action": "login", "timestamp": datetime.utcnow().isoformat()})
    return {"access_token": access_token, "token_type": "bearer"}

@app.post("/profile", dependencies=[Depends(get_current_user)])
def get_profile(user=Depends(get_current_user)):
    AUDIT_LOG.append({"user": user["sub"], "action": "view_profile", "timestamp": datetime.utcnow().isoformat()})
    return {"username": user["sub"], "role": user["role"]}

@app.post("/password-reset-request")
def password_reset_request(username: str = Body(...), background_tasks: BackgroundTasks = None):
    if username not in USER_DB:
        return {"error": "User not found"}
    token = str(uuid.uuid4())
    PASSWORD_RESET[username] = token
    if background_tasks:
        background_tasks.add_task(print, f"Send password reset email to {username} with token {token}")
    AUDIT_LOG.append({"user": username, "action": "password_reset_requested", "timestamp": datetime.utcnow().isoformat()})
    return {"msg": "Password reset email sent"}

@app.post("/password-reset")
def password_reset(username: str = Body(...), token: str = Body(...), new_password: str = Body(...)):
    if PASSWORD_RESET.get(username) != token:
        return {"error": "Invalid token"}
    USER_DB[username]["password"] = pwd_context.hash(new_password)
    AUDIT_LOG.append({"user": username, "action": "password_reset", "timestamp": datetime.utcnow().isoformat()})
    return {"msg": "Password updated"}

@app.post("/verify-email")
def verify_email(username: str = Body(...), background_tasks: BackgroundTasks = None):
    if username not in USER_DB:
        return {"error": "User not found"}
    token = str(uuid.uuid4())
    EMAIL_VERIFICATION[username] = token
    if background_tasks:
        background_tasks.add_task(print, f"Send verification email to {username} with token {token}")
    AUDIT_LOG.append({"user": username, "action": "email_verification_requested", "timestamp": datetime.utcnow().isoformat()})
    return {"msg": "Verification email sent"}

@app.post("/confirm-email")
def confirm_email(username: str = Body(...), token: str = Body(...)):
    if EMAIL_VERIFICATION.get(username) != token:
        return {"error": "Invalid token"}
    USER_DB[username]["verified"] = True
    AUDIT_LOG.append({"user": username, "action": "email_verified", "timestamp": datetime.utcnow().isoformat()})
    return {"msg": "Email verified"}

@app.get("/audit-log", dependencies=[Depends(require_role("admin"))])
def get_audit_log():
    return AUDIT_LOG


# ============================================================================
# SyncScrape Integration
# ============================================================================

class ScrapeRequest(BaseModel):
    """Request model for brand scraping and campaign generation"""
    url: str = Field(..., description="Target website URL to scrape", json_schema_extra={"example": "https://example.com"})
    campaign_goal: str = Field(default="brand awareness", description="Campaign objective", json_schema_extra={"example": "customer acquisition"})
    num_ad_copies: int = Field(default=5, ge=1, le=10, description="Number of ad copy variations to generate")
    num_image_prompts: int = Field(default=3, ge=1, le=5, description="Number of image prompts to generate")


@app.post("/scrape-and-generate", tags=["scraper"], responses={
    200: {"description": "Brand identity extracted and campaign assets generated"},
    400: {"description": "Invalid URL or parameters"},
    500: {"description": "Scraping or generation error"}
})
async def scrape_and_generate(req: ScrapeRequest):
    """
    Extract brand identity from a URL and generate campaign assets.
    
    This endpoint orchestrates the complete SyncScrape workflow:
    1. Extract brand identity (colors, tone, products) using Playwright + BeautifulSoup
    2. Generate campaign brief via SyncBrain LLM
    3. Create ad copies and image prompts via SyncCreate
    4. Validate all assets through SyncShield for brand safety
    
    Returns comprehensive brand analysis and validated creative assets.
    """
    REQUEST_COUNT.labels(endpoint="/scrape-and-generate", method="POST").inc()
    
    with REQUEST_LATENCY.labels(endpoint="/scrape-and-generate").time():
        with tracer.start_as_current_span("scrape_and_generate"):
            try:
                # Lazy import to avoid startup dependency
                import sys
                import os
                sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))
                from scraper import SyncScrapeOrchestrator
                
                # Initialize orchestrator
                orchestrator = SyncScrapeOrchestrator(
                    syncbrain_url=config.syncbrain_url,
                    synccreate_url=config.synccreate_url,
                    syncshield_url=config.syncshield_url
                )
                
                logger.info(f"Starting SyncScrape for URL: {req.url}")
                
                # Execute workflow
                result = await orchestrator.execute(
                    url=req.url,
                    campaign_goal=req.campaign_goal,
                    num_ad_copies=req.num_ad_copies,
                    num_image_prompts=req.num_image_prompts
                )
                
                logger.info(f"SyncScrape completed: {result['metrics']['approval_rate']:.1f}% approval rate")
                
                return result
                
            except Exception as e:
                logger.error(f"Error in scrape-and-generate: {e}", exc_info=True)
                ERROR_COUNT.labels(endpoint="/scrape-and-generate").inc()
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to scrape and generate: {str(e)}"
                )


# ============================================================================
# Council of Nine - URL-to-Campaign Orchestration
# ============================================================================

class URLToCampaignRequest(BaseModel):
    """Request model for URL-to-Campaign automation"""
    url: str = Field(..., description="Target business website URL")
    auto_deploy: bool = Field(default=False, description="Automatically deploy to SyncFlow")


@app.post("/api/v1/url-to-campaign", tags=["council-of-nine"], summary="Transform URL into deployed campaign")
async def url_to_campaign(req: URLToCampaignRequest):
    """
    ðŸš€ Council of Nine: URL-to-Campaign Automation
    
    Transform a website URL into a complete, deployed advertising campaign.
    
    This implements KIKI's "Advantage+ Suite":
    - **Advantage+ Audience** â†’ SyncValue LTV prediction + SyncBrain targeting
    - **Advantage+ Creative** â†’ SyncCreate autonomous generation + SyncShield safety
    - **Advantage+ Placements** â†’ SyncFlow LTV-optimized bidding
    
    Orchestration Flow:
    1. **SyncValue**: Scrape URL â†’ Extract brand identity â†’ Classify industry â†’ Predict LTV baseline
    2. **SyncBrain**: Generate campaign strategy based on industry and audience
    3. **SyncCreate**: Generate 5 ad copies + 3 image prompts (on-brand, autonomous)
    4. **SyncShield**: Validate all assets for brand safety and compliance
    5. **SyncFlow**: Deploy campaign with LTV-based bidding constraints (if auto_deploy=true)
    6. **SyncEngage**: Setup retention triggers (future enhancement)
    
    Returns complete campaign deployment with all generated assets.
    """
    REQUEST_COUNT.labels(endpoint="/url-to-campaign", method="POST").inc()
    
    with REQUEST_LATENCY.labels(endpoint="/url-to-campaign").time():
        with tracer.start_as_current_span("url_to_campaign"):
            try:
                # Lazy import
                import sys
                import os
                sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))
                from council_of_nine import CouncilOfNine
                
                logger.info(f"ðŸš€ Council of Nine: Starting URL-to-Campaign for {req.url}")
                
                # Initialize orchestrator
                council = CouncilOfNine(
                    syncvalue_url=f"http://localhost:{config.port}",  # Self
                    syncbrain_url=config.syncbrain_url,
                    synccreate_url=config.synccreate_url,
                    syncshield_url=config.syncshield_url,
                    syncflow_url=config.syncflow_url
                )
                
                # Execute orchestration
                deployment = await council.url_to_campaign(
                    url=req.url,
                    auto_deploy=req.auto_deploy
                )
                
                logger.info(f"âœ… Campaign deployment complete: {deployment.deployment_id}")
                logger.info(f"   Brand: {deployment.brand_name}")
                logger.info(f"   Industry: {deployment.industry_category}")
                logger.info(f"   Assets: {len(deployment.ad_copies)} ad copies, {len(deployment.image_prompts)} image prompts")
                logger.info(f"   Status: {deployment.deployment_status}")
                
                return {
                    "success": True,
                    "deployment": deployment.to_dict(),
                    "message": f"Campaign created successfully for {deployment.brand_name}",
                    "next_steps": [
                        "Review generated ad copies and image prompts",
                        "Deploy to SyncFlow when ready (if not auto-deployed)",
                        "Monitor performance via SyncPortal dashboard"
                    ]
                }
                
            except Exception as e:
                logger.error(f"Error in URL-to-Campaign: {e}", exc_info=True)
                ERROR_COUNT.labels(endpoint="/url-to-campaign").inc()
                raise HTTPException(
                    status_code=500,
                    detail=f"Council of Nine orchestration failed: {str(e)}"
                )


class PromptToCampaignRequest(BaseModel):
    """Request model for Prompt-to-Campaign automation"""
    prompt: str = Field(..., description="Natural language campaign description (e.g., 'Launch product with $100k budget, target ROI 3x')")
    auto_deploy: bool = Field(default=False, description="Automatically deploy to SyncFlow")


@app.post("/api/v1/prompt-to-campaign", tags=["council-of-nine"], summary="Transform prompt into deployed campaign")
async def prompt_to_campaign(req: PromptToCampaignRequest):
    """
    ðŸš€ Council of Nine: Prompt-to-Campaign Automation
    
    Transform a natural language prompt into a complete, deployed advertising campaign.
    
    **Example Prompts:**
    - "Launch product with $100k budget, target ROI 3x"
    - "Create campaign for SaaS startup, B2B enterprise audience"
    - "Mobile game launch, casual players, $50k budget"
    - "E-commerce store targeting millennials, $25k budget"
    
    This is the **primary entry point** for users who want to describe their campaign goals
    in natural language without needing a website URL.
    
    Orchestration Flow:
    1. **SyncBrain**: Parse prompt â†’ Extract budget, ROI, industry, audience, brand
    2. **SyncValue**: Predict LTV baseline â†’ Calculate max CPA from ROI target
    3. **SyncBrain**: Generate campaign strategy based on parsed intent
    4. **SyncCreate**: Generate 5 ad copies + 3 image prompts
    5. **SyncShield**: Validate all assets for brand safety and compliance
    6. **SyncFlow**: Deploy campaign with budget & ROI constraints (if auto_deploy=true)
    
    Returns complete campaign deployment with all generated assets.
    """
    REQUEST_COUNT.labels(endpoint="/prompt-to-campaign", method="POST").inc()
    
    with REQUEST_LATENCY.labels(endpoint="/prompt-to-campaign").time():
        with tracer.start_as_current_span("prompt_to_campaign"):
            try:
                # Lazy import
                import sys
                import os
                sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))
                from council_of_nine import CouncilOfNine
                
                logger.info(f"ðŸš€ Council of Nine: Starting Prompt-to-Campaign")
                logger.info(f"   Prompt: {req.prompt}")
                
                # Initialize orchestrator
                council = CouncilOfNine(
                    syncvalue_url=f"http://localhost:{config.port}",  # Self
                    syncbrain_url=config.syncbrain_url,
                    synccreate_url=config.synccreate_url,
                    syncshield_url=config.syncshield_url,
                    syncflow_url=config.syncflow_url
                )
                
                # Execute orchestration
                deployment = await council.prompt_to_campaign(
                    prompt=req.prompt,
                    auto_deploy=req.auto_deploy
                )
                
                logger.info(f"âœ… Campaign deployment complete: {deployment.deployment_id}")
                logger.info(f"   Brand: {deployment.brand_name}")
                logger.info(f"   Industry: {deployment.industry_category}")
                if deployment.budget:
                    logger.info(f"   Budget: ${deployment.budget:,.0f}")
                if deployment.target_roi:
                    logger.info(f"   Target ROI: {deployment.target_roi}x")
                if deployment.max_cpa:
                    logger.info(f"   Max CPA: ${deployment.max_cpa:.2f}")
                logger.info(f"   Assets: {len(deployment.ad_copies)} ad copies, {len(deployment.image_prompts)} image prompts")
                logger.info(f"   Status: {deployment.deployment_status}")
                
                return {
                    "success": True,
                    "deployment": deployment.to_dict(),
                    "message": f"Campaign created successfully for {deployment.brand_name}",
                    "financial_summary": {
                        "budget": deployment.budget,
                        "target_roi": deployment.target_roi,
                        "ltv_baseline": deployment.ltv_baseline,
                        "max_cpa": deployment.max_cpa,
                        "roi_calculation": f"LTV ${deployment.ltv_baseline:.2f} / Target ROI {deployment.target_roi}x = Max CPA ${deployment.max_cpa:.2f}" if deployment.max_cpa else None
                    },
                    "next_steps": [
                        "Review generated ad copies and image prompts",
                        "Verify budget and ROI constraints",
                        "Deploy to SyncFlow when ready (if not auto-deployed)",
                        "Monitor performance via SyncPortal dashboard"
                    ]
                }
                
            except Exception as e:
                logger.error(f"Error in Prompt-to-Campaign: {e}", exc_info=True)
                ERROR_COUNT.labels(endpoint="/prompt-to-campaign").inc()
                raise HTTPException(
                    status_code=500,
                    detail=f"Council of Nine orchestration failed: {str(e)}"
                )


@app.get("/api/v1/advantage-plus-info", tags=["council-of-nine"], summary="Get Advantage+ Suite information")
async def advantage_plus_info():
    """
    Information about KIKI's Advantage+ Suite
    
    Maps Meta's automation features to KIKI agents.
    """
    return {
        "advantage_plus_suite": {
            "advantage_plus_audience": {
                "meta_feature": "Finds audiences beyond initial targeting to lower costs",
                "kiki_enhancement": "SyncValueâ„¢ predicts actual long-term value of audiences. SyncBrainâ„¢ orchestrates targeting strategies based on highest predicted retention.",
                "agents": ["SyncValue", "SyncBrain"],
                "advantage": "LTV-based targeting instead of just cost-per-acquisition"
            },
            "advantage_plus_creative": {
                "meta_feature": "Automatically swaps creative assets to improve performance",
                "kiki_enhancement": "SyncCreateâ„¢ generates new on-brand creatives autonomously using Stable Diffusion. SyncShieldâ„¢ ensures every version meets enterprise brand safety standards.",
                "agents": ["SyncCreate", "SyncShield"],
                "advantage": "Autonomous creative generation, not just asset swapping"
            },
            "advantage_plus_placements": {
                "meta_feature": "Automatically finds cheapest placements across Facebook, Instagram, etc.",
                "kiki_enhancement": "SyncFlowâ„¢ handles sub-millisecond execution across networks. It doesn't just find 'cheapest' placeâ€”it finds the placement most likely to convert a high-LTV customer.",
                "agents": ["SyncFlow"],
                "advantage": "LTV-optimized placement selection with <1ms bidding"
            }
        },
        "industry_targeting": {
            "optimal_industries": [
                {
                    "name": "E-commerce & Retail",
                    "reason": "SyncCreate handles high-volume creative refreshes, SyncEngage manages post-purchase loyalty",
                    "campaign_type": "ASC (Advantage+ Shopping Campaigns)",
                    "ltv_baseline": "$75-250"
                },
                {
                    "name": "Subscription SaaS",
                    "reason": "SyncValue identifies at-risk users early to trigger SyncShield-protected retention offers",
                    "campaign_type": "ALC (Advantage+ Lead Campaigns)",
                    "ltv_baseline": "$150-2500"
                },
                {
                    "name": "Mobile Gaming",
                    "reason": "SyncFlow bids aggressively for 'whales' (high-spenders) based on first-session behavior predictions",
                    "campaign_type": "ASC (App Install)",
                    "ltv_baseline": "$25-100"
                }
            ]
        },
        "usage": {
            "url_to_campaign": "POST /api/v1/url-to-campaign with {url: 'https://example.com'}",
            "industry_discovery": "Automatically classifies industry from URL and selects optimal campaign type"
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=config.port)

